# AIç ”å‘è¾…åŠ©å¹³å° - åç«¯éƒ¨ç½²æŒ‡å—

## ğŸš€ éƒ¨ç½²æ¦‚è¿°

### éƒ¨ç½²æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è´Ÿè½½å‡è¡¡å™¨ (Nginx)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FastAPIåº”ç”¨ (å¤šå®ä¾‹)  â”‚  Celery Worker  â”‚  Celery Beat     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL (ä¸»ä»)     â”‚  Redis Cluster  â”‚  ChromaDB        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ–‡ä»¶å­˜å‚¨ (MinIO/S3)   â”‚  æ—¥å¿—æ”¶é›†       â”‚  ç›‘æ§å‘Šè­¦        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç¯å¢ƒè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / CentOS 8+ / Docker
- **Python**: 3.11+
- **PostgreSQL**: 15+
- **Redis**: 7+
- **å†…å­˜**: æœ€å°4GBï¼Œæ¨è8GB+
- **å­˜å‚¨**: æœ€å°50GBï¼Œæ¨è100GB+

## ğŸ³ Dockeréƒ¨ç½²

### 1. é¡¹ç›®ç»“æ„
```
backend/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Dockerfile.celery
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ docker-compose.prod.yml
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init-db.sh
â”‚   â”œâ”€â”€ backup-db.sh
â”‚   â””â”€â”€ deploy.sh
â”œâ”€â”€ .env.example
â””â”€â”€ requirements.txt
```

### 2. Dockerfile
```dockerfile
# docker/Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    libmagic1 \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p /app/uploads /app/logs /app/chroma_db && \
    chown -R appuser:appuser /app

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 3. Celery Dockerfile
```dockerfile
# docker/Dockerfile.celery
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    git \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r celeryuser && useradd -r -g celeryuser celeryuser

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p /app/logs && \
    chown -R celeryuser:celeryuser /app

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER celeryuser

# å¯åŠ¨å‘½ä»¤ï¼ˆå°†åœ¨docker-composeä¸­è¦†ç›–ï¼‰
CMD ["celery", "-A", "app.tasks.celery_app", "worker", "--loglevel=info"]
```

### 4. Docker Compose (å¼€å‘ç¯å¢ƒ)
```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:password@postgres:5432/ai_platform
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
      - ./chroma_db:/app/chroma_db
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  celery-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.celery
    command: celery -A app.tasks.celery_app worker --loglevel=info --concurrency=4
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:password@postgres:5432/ai_platform
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    restart: unless-stopped

  celery-beat:
    build:
      context: .
      dockerfile: docker/Dockerfile.celery
    command: celery -A app.tasks.celery_app beat --loglevel=info
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:password@postgres:5432/ai_platform
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - api
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:

networks:
  default:
    driver: bridge
```

### 5. ç”Ÿäº§ç¯å¢ƒ Docker Compose
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  api:
    image: ai-platform-api:latest
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres:5432/ai_platform
      - REDIS_URL=redis://redis:6379/0
      - SECRET_KEY=${SECRET_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
    volumes:
      - uploads_data:/app/uploads
      - logs_data:/app/logs
      - chroma_data:/app/chroma_db
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  celery-worker:
    image: ai-platform-celery:latest
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    command: celery -A app.tasks.celery_app worker --loglevel=info --concurrency=8
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD}@postgres:5432/ai_platform
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    volumes:
      - uploads_data:/app/uploads
      - logs_data:/app/logs
    networks:
      - backend

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=ai_platform
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
    networks:
      - backend
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100

  redis:
    image: redis:7-alpine
    command: >
      redis-server
      --appendonly yes
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis_data:/data
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    networks:
      - backend

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  uploads_data:
    driver: local
  logs_data:
    driver: local
  chroma_data:
    driver: local

networks:
  backend:
    driver: overlay
    attachable: true
```

### 6. Nginxé…ç½®
```nginx
# docker/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream api_backend {
        least_conn;
        server api:8000 max_fails=3 fail_timeout=30s;
    }

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=upload_limit:10m rate=1r/s;

    # æ—¥å¿—æ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # åŸºç¡€é…ç½®
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 50M;

    # Gzipå‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json;

    server {
        listen 80;
        server_name api.yourdomain.com;

        # é‡å®šå‘åˆ°HTTPS
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name api.yourdomain.com;

        # SSLé…ç½®
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;

        # å®‰å…¨å¤´
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
        add_header X-Frame-Options "DENY" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;

        # APIè·¯ç”±
        location /api/ {
            limit_req zone=api_limit burst=20 nodelay;
            
            proxy_pass http://api_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 30s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # ç¼“å†²è®¾ç½®
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # æ–‡ä»¶ä¸Šä¼ è·¯ç”±
        location /api/v1/upload {
            limit_req zone=upload_limit burst=5 nodelay;
            
            proxy_pass http://api_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # ä¸Šä¼ ä¸“ç”¨è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 30s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
            
            client_max_body_size 50M;
        }

        # å¥åº·æ£€æŸ¥
        location /health {
            proxy_pass http://api_backend;
            access_log off;
        }

        # é™æ€æ–‡ä»¶
        location /static/ {
            alias /app/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }
}
```

## ğŸ”§ éƒ¨ç½²è„šæœ¬

### 1. åˆå§‹åŒ–è„šæœ¬
```bash
#!/bin/bash
# scripts/init-db.sh

set -e

echo "åˆå§‹åŒ–æ•°æ®åº“..."

# ç­‰å¾…PostgreSQLå¯åŠ¨
until pg_isready -h postgres -p 5432 -U postgres; do
  echo "ç­‰å¾…PostgreSQLå¯åŠ¨..."
  sleep 2
done

# åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
psql -h postgres -U postgres -tc "SELECT 1 FROM pg_database WHERE datname = 'ai_platform'" | grep -q 1 || psql -h postgres -U postgres -c "CREATE DATABASE ai_platform"

echo "æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ"
```

### 2. éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½²AIç ”å‘è¾…åŠ©å¹³å°åç«¯..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ ! -f .env ]; then
    echo "âŒ .envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºç¯å¢ƒé…ç½®"
    exit 1
fi

# åŠ è½½ç¯å¢ƒå˜é‡
source .env

# æ„å»ºé•œåƒ
echo "ğŸ“¦ æ„å»ºDockeré•œåƒ..."
docker build -t ai-platform-api:latest -f docker/Dockerfile .
docker build -t ai-platform-celery:latest -f docker/Dockerfile.celery .

# åœæ­¢æ—§æœåŠ¡
echo "ğŸ›‘ åœæ­¢æ—§æœåŠ¡..."
docker-compose -f docker-compose.prod.yml down

# å¤‡ä»½æ•°æ®åº“
echo "ğŸ’¾ å¤‡ä»½æ•°æ®åº“..."
./scripts/backup-db.sh

# å¯åŠ¨æ–°æœåŠ¡
echo "ğŸ”„ å¯åŠ¨æ–°æœåŠ¡..."
docker-compose -f docker-compose.prod.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# è¿è¡Œæ•°æ®åº“è¿ç§»
echo "ğŸ”„ è¿è¡Œæ•°æ®åº“è¿ç§»..."
docker-compose -f docker-compose.prod.yml exec api alembic upgrade head

# å¥åº·æ£€æŸ¥
echo "ğŸ” å¥åº·æ£€æŸ¥..."
if curl -f http://localhost/health; then
    echo "âœ… éƒ¨ç½²æˆåŠŸï¼"
else
    echo "âŒ éƒ¨ç½²å¤±è´¥ï¼Œæ­£åœ¨å›æ»š..."
    docker-compose -f docker-compose.prod.yml down
    # è¿™é‡Œå¯ä»¥æ·»åŠ å›æ»šé€»è¾‘
    exit 1
fi

echo "ğŸ‰ éƒ¨ç½²å®Œæˆï¼"
```

### 3. å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# scripts/backup-db.sh

set -e

BACKUP_DIR="/backups"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="ai_platform_backup_${DATE}.sql"

echo "ğŸ“¦ å¼€å§‹å¤‡ä»½æ•°æ®åº“..."

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½æ•°æ®åº“
docker-compose exec postgres pg_dump -U postgres ai_platform > "${BACKUP_DIR}/${BACKUP_FILE}"

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip "${BACKUP_DIR}/${BACKUP_FILE}"

echo "âœ… æ•°æ®åº“å¤‡ä»½å®Œæˆ: ${BACKUP_FILE}.gz"

# æ¸…ç†7å¤©å‰çš„å¤‡ä»½
find $BACKUP_DIR -name "*.gz" -mtime +7 -delete

echo "ğŸ§¹ æ¸…ç†æ—§å¤‡ä»½å®Œæˆ"
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### 1. æ—¥å¿—é…ç½®
```yaml
# docker-compose.yml ä¸­çš„æ—¥å¿—é…ç½®
logging:
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    labels: "service,version"
```

### 2. å¥åº·æ£€æŸ¥
```python
# app/health.py
from fastapi import APIRouter
from sqlmodel import Session, text
from app.database import get_db
from app.tasks.celery_app import celery_app
import redis
import time

router = APIRouter()

@router.get("/health")
async def health_check():
    """ç»¼åˆå¥åº·æ£€æŸ¥"""
    checks = {
        "api": "healthy",
        "database": await check_database(),
        "redis": await check_redis(),
        "celery": await check_celery(),
        "timestamp": time.time()
    }
    
    # å¦‚æœä»»ä½•ç»„ä»¶ä¸å¥åº·ï¼Œè¿”å›503
    if any(status != "healthy" for status in checks.values() if status != checks["timestamp"]):
        return JSONResponse(status_code=503, content=checks)
    
    return checks

async def check_database():
    """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
    try:
        db = next(get_db())
        result = await db.execute(text("SELECT 1"))
        return "healthy"
    except Exception:
        return "unhealthy"

async def check_redis():
    """æ£€æŸ¥Redisè¿æ¥"""
    try:
        r = redis.Redis.from_url(settings.REDIS_URL)
        r.ping()
        return "healthy"
    except Exception:
        return "unhealthy"

async def check_celery():
    """æ£€æŸ¥CeleryçŠ¶æ€"""
    try:
        inspect = celery_app.control.inspect()
        stats = inspect.stats()
        if stats:
            return "healthy"
        return "unhealthy"
    except Exception:
        return "unhealthy"
```

### 3. æ€§èƒ½ç›‘æ§
```python
# app/middleware.py
import time
import structlog
from fastapi import Request, Response

logger = structlog.get_logger()

async def performance_middleware(request: Request, call_next):
    """æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶"""
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    
    logger.info(
        "request_processed",
        method=request.method,
        url=str(request.url),
        status_code=response.status_code,
        process_time=process_time,
        user_agent=request.headers.get("user-agent"),
        ip=request.client.host
    )
    
    response.headers["X-Process-Time"] = str(process_time)
    return response
```

## ğŸ”’ å®‰å…¨é…ç½®

### 1. ç¯å¢ƒå˜é‡
```bash
# .env.prod
# åº”ç”¨é…ç½®
APP_NAME=AIç ”å‘è¾…åŠ©å¹³å°
DEBUG=False
SECRET_KEY=your-super-secret-key-here
ENVIRONMENT=production

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql+asyncpg://postgres:your-strong-password@postgres:5432/ai_platform
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=30

# JWTé…ç½®
JWT_SECRET_KEY=your-jwt-secret-key-here
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# åŠ å¯†é…ç½®
ENCRYPTION_KEY=your-fernet-encryption-key-here

# Redisé…ç½®
REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/1
CELERY_RESULT_BACKEND=redis://redis:6379/2

# AIæœåŠ¡é…ç½®ï¼ˆæ ¹æ®éœ€è¦é…ç½®ï¼‰
OPENAI_API_KEY=your-openai-api-key
DEEPSEEK_API_KEY=your-deepseek-api-key

# æ–‡ä»¶å­˜å‚¨é…ç½®
UPLOAD_DIR=/app/uploads
MAX_FILE_SIZE=52428800  # 50MB

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
SENTRY_DSN=your-sentry-dsn-here
```

### 2. é˜²ç«å¢™é…ç½®
```bash
# åªå¼€æ”¾å¿…è¦ç«¯å£
ufw allow 22/tcp    # SSH
ufw allow 80/tcp    # HTTP
ufw allow 443/tcp   # HTTPS
ufw enable
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2024å¹´12æœˆ  
**è´Ÿè´£äºº**: è¿ç»´å›¢é˜Ÿ
